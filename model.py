# -*- coding: utf-8 -*-
"""CTS_with_LIME.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJn-wYY8FEGh4_YmxzqtI6wtoPcPwyeu
"""



import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import random
import joblib
import json
from collections import Counter

# Try to import optional libraries
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    print("XGBoost not available, will use RandomForest as best alternative")
    XGB_AVAILABLE = False

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAP not available, explanations will be limited")
    SHAP_AVAILABLE = False

try:
    from lime.lime_tabular import LimeTabularExplainer
    LIME_AVAILABLE = True
except ImportError:
    print("LIME not available, will use SHAP for explanations if available")
    LIME_AVAILABLE = False

try:
    from imblearn.over_sampling import SMOTE
    SMOTE_AVAILABLE = True
except ImportError:
    print("imbalanced-learn not available, will skip SMOTE oversampling")
    SMOTE_AVAILABLE = False

"""**Data Cleaning**"""

# Data loading with error handling
try:
    Test = pd.read_csv("./content/Test-1542969243754.csv")
    Test_Inpatientdata = pd.read_csv("./content/Test_Inpatientdata-1542969243754.csv")
    Test_Outpatientdata = pd.read_csv("./content/Test_Outpatientdata-1542969243754.csv")
    Test_Beneficiarydata = pd.read_csv("./content/Test_Beneficiarydata-1542969243754.csv")

    Train = pd.read_csv("./content/Train-1542865627584.csv")
    Train_Beneficiarydata = pd.read_csv("./content/Train_Beneficiarydata-1542865627584.csv")
    Train_Inpatientdata = pd.read_csv("./content/Train_Inpatientdata-1542865627584.csv")
    Train_Outpatientdata = pd.read_csv("./content/Train_Outpatientdata-1542865627584.csv")

    print("Data files loaded successfully")
except FileNotFoundError as e:
    print(f"Error loading data files: {e}")
    print("Please ensure the CSV files are in the /content/ directory")
    raise
except Exception as e:
    print(f"Unexpected error loading data: {e}")
    raise

print("The Training inpatient: {} rows and {} columns. \n" .format(Train_Inpatientdata.shape[0], Train_Inpatientdata.shape[1]))
print("The Training outpatient: {} rows and {} columns. \n" .format(Train_Outpatientdata.shape[0], Train_Outpatientdata.shape[1]))
print("The Training Benficiary: {} rows and {} columns. \n" .format(Train_Beneficiarydata.shape[0], Train_Beneficiarydata.shape[1]))

print("The Test inpatient: {} rows and {} columns. \n" .format(Test_Inpatientdata.shape[0], Test_Inpatientdata.shape[1]))
print("The Test outpatient: {} rows and {} columns. \n" .format(Test_Outpatientdata.shape[0], Test_Outpatientdata.shape[1]))
print("The Test Benficiary: {} rows and {} columns. \n" .format(Test_Beneficiarydata.shape[0], Test_Beneficiarydata.shape[1]))

"""**Beneficiarydata**"""

Train_Beneficiarydata.head()

Train_Beneficiarydata.isna().sum()

print(Train_Beneficiarydata.duplicated().sum())
print(Test_Beneficiarydata.duplicated().sum())

Train_Beneficiarydata['DOB'] = pd.to_datetime(Train_Beneficiarydata['DOB'], format='%Y-%m-%d')
Train_Beneficiarydata['DOD'] = pd.to_datetime(Train_Beneficiarydata['DOD'], format='%Y-%m-%d', errors='ignore')
Test_Beneficiarydata['DOB'] = pd.to_datetime(Test_Beneficiarydata['DOB'], format='%Y-%m-%d')
Test_Beneficiarydata['DOD'] = pd.to_datetime(Test_Beneficiarydata['DOD'], format='%Y-%m-%d', errors='ignore')

Train_Beneficiarydata['Age'] = round((Train_Beneficiarydata['DOD'] - Train_Beneficiarydata['DOB']).dt.days / 365)
Test_Beneficiarydata['Age'] = round((Test_Beneficiarydata['DOD'] - Test_Beneficiarydata['DOB']).dt.days / 365)

Train_Beneficiarydata['Age'] = Train_Beneficiarydata['Age'].fillna(round((pd.to_datetime('2009-12-01') - Train_Beneficiarydata['DOB']).dt.days / 365))
Test_Beneficiarydata['Age'] = Test_Beneficiarydata['Age'].fillna(round((pd.to_datetime('2009-12-01') - Test_Beneficiarydata['DOB']).dt.days / 365))

Train_Beneficiarydata['AliveorDead'] = Train_Beneficiarydata['DOD'].notna().astype(int)
Test_Beneficiarydata['AliveorDead'] = Test_Beneficiarydata['DOD'].notna().astype(int)

Train_Beneficiarydata.groupby(['AliveorDead'])['BeneID'].nunique()

"""**Inpatientdata**"""

Train_Inpatientdata.head()

Train_Inpatientdata.isna().sum()

Train_Inpatientdata['AdmissionDt'] = pd.to_datetime(Train_Inpatientdata['AdmissionDt'], format='%Y-%m-%d')
Train_Inpatientdata['DischargeDt'] = pd.to_datetime(Train_Inpatientdata['DischargeDt'], format='%Y-%m-%d')
Test_Inpatientdata['AdmissionDt'] = pd.to_datetime(Test_Inpatientdata['AdmissionDt'], format='%Y-%m-%d')
Test_Inpatientdata['DischargeDt'] = pd.to_datetime(Test_Inpatientdata['DischargeDt'], format='%Y-%m-%d')

Train_Inpatientdata['NumberofDaysAdmitted'] = (Train_Inpatientdata['DischargeDt'] - Train_Inpatientdata['AdmissionDt']).dt.days + 1
Test_Inpatientdata['NumberofDaysAdmitted'] = (Test_Inpatientdata['DischargeDt'] - Test_Inpatientdata['AdmissionDt']).dt.days + 1

Train_Inpatientdata['ClaimEndDt'] = pd.to_datetime(Train_Inpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Train_Inpatientdata['ClaimStartDt'] = pd.to_datetime(Train_Inpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Test_Inpatientdata['ClaimEndDt'] = pd.to_datetime(Test_Inpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Test_Inpatientdata['ClaimStartDt'] = pd.to_datetime(Test_Inpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Train_Inpatientdata['DurationofClaim'] = (Train_Inpatientdata['ClaimEndDt'] - Train_Inpatientdata['ClaimStartDt']).dt.days
Test_Inpatientdata['DurationofClaim'] = (Test_Inpatientdata['ClaimEndDt'] - Test_Inpatientdata['ClaimStartDt']).dt.days

Train_Inpatientdata['Admitted'] =1
Test_Inpatientdata['Admitted'] =1

px.histogram(Train_Inpatientdata, x = 'NumberofDaysAdmitted', title='NumberofDaysAdmitted',
    width=600,
    height=400)

px.box(Train_Inpatientdata, x = 'NumberofDaysAdmitted',
    width=600,
    height=400)

px.histogram(Train_Inpatientdata, x = 'DurationofClaim', title='DurationofClaim',width=600,height=400)

"""**Outpatientdata**"""

Train_Outpatientdata.head()

print(Train_Outpatientdata.duplicated().sum())
print(Test_Outpatientdata.duplicated().sum())

Train_Outpatientdata.isnull().sum()

Train_Outpatientdata['Admitted'] = 0
Test_Outpatientdata['Admitted'] = 0

Train_Outpatientdata['ClaimEndDt'] = pd.to_datetime(Train_Outpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Train_Outpatientdata['ClaimStartDt'] = pd.to_datetime(Train_Outpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Test_Outpatientdata['ClaimEndDt'] = pd.to_datetime(Test_Outpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Test_Outpatientdata['ClaimStartDt'] = pd.to_datetime(Test_Outpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Train_Outpatientdata['DurationofClaim'] = (Train_Outpatientdata['ClaimEndDt'] - Train_Outpatientdata['ClaimStartDt']).dt.days
Test_Outpatientdata['DurationofClaim'] = (Test_Outpatientdata['ClaimEndDt'] - Test_Outpatientdata['ClaimStartDt']).dt.days

"""*Merging*"""

common_cols = list(set(Train_Inpatientdata.columns).intersection(set(Train_Outpatientdata.columns)))

print(common_cols)

Train_Allpatientdata = pd.merge(Train_Outpatientdata, Train_Inpatientdata, on=common_cols, how='outer')
Test_Allpatientdata = pd.merge(Test_Outpatientdata, Test_Inpatientdata, on=common_cols, how='outer')

print(Train_Allpatientdata.shape)
print(Test_Allpatientdata.shape)

df_train = Train_Allpatientdata.merge(Train_Beneficiarydata, on='BeneID', how='inner')
df_test = Test_Allpatientdata.merge(Test_Beneficiarydata, on='BeneID', how='inner')

print('Training data shape: ', df_train.shape)
print('Test data shape: ', df_test.shape)

df_train1 = pd.merge(Train, df_train,on='Provider')
df_test1 = pd.merge(Test, df_test,on='Provider')

# Remove duplicates to prevent data leakage
print(f"Rows before deduplication: {df_train1.shape[0]}")
df_train1 = df_train1.drop_duplicates()
print(f"Rows after deduplication: {df_train1.shape[0]}")

df_train1['RenalDiseaseIndicator'] = df_train1['RenalDiseaseIndicator'].replace('Y','1')
df_train1['RenalDiseaseIndicator'] = df_train1['RenalDiseaseIndicator'].astype(int)

df_test1['RenalDiseaseIndicator'] = df_test1['RenalDiseaseIndicator'].replace('Y','1')
df_test1['RenalDiseaseIndicator'] = df_test1['RenalDiseaseIndicator'].astype(int)

df_train1 = df_train1.drop(columns=['DOB', 'DOD'], axis=1)
df_test1 = df_test1.drop(columns=['DOB', 'DOD'], axis=1)

df_train1['ClmDiagnosisCodeIndex'] = df_train1.filter(regex='ClmDiagnosisCode_').notnull().sum(axis=1)
df_test1['ClmDiagnosisCodeIndex'] = df_test1.filter(regex='ClmDiagnosisCode_').notnull().sum(axis=1)

df_train1['ClmProcedureCodeIndex'] = df_train1.filter(regex='ClmProcedureCode_').notnull().sum(axis=1)
df_test1['ClmProcedureCodeIndex'] = df_test1.filter(regex='ClmProcedureCode_').notnull().sum(axis=1)

columns_to_drop = df_train1.filter(regex='ClmProcedureCode_|ClmDiagnosisCode_').columns
df_train1 = df_train1.drop(columns_to_drop, axis=1)
df_test1 = df_test1.drop(columns_to_drop, axis=1)

df_train1['NumberofDaysAdmitted'] = df_train1['NumberofDaysAdmitted'].fillna(0)
df_test1['NumberofDaysAdmitted'] = df_test1['NumberofDaysAdmitted'].fillna(0)

df_train1 = df_train1.dropna(subset=['AttendingPhysician'])
df_test1 = df_test1.dropna(subset=['AttendingPhysician'])

df_train1['DeductibleAmtPaid'] = df_train1['DeductibleAmtPaid'].fillna(df_train1['DeductibleAmtPaid'].mean())
df_test1['DeductibleAmtPaid'] = df_test1['DeductibleAmtPaid'].fillna(df_test1['DeductibleAmtPaid'].mean())

columns_to_transform = ["InscClaimAmtReimbursed", "DeductibleAmtPaid", "IPAnnualReimbursementAmt", "IPAnnualDeductibleAmt","OPAnnualReimbursementAmt", "OPAnnualDeductibleAmt", "Age", "NoOfMonths_PartACov", "NoOfMonths_PartBCov","DurationofClaim","NumberofDaysAdmitted"]

for column in columns_to_transform:
    df_train1[f"PerProviderAvg_{column}"] = df_train1.groupby('Provider')[column].transform('mean')
    df_test1[f"PerProviderAvg_{column}"] = df_test1.groupby('Provider')[column].transform('mean')

columns_to_transform = [
    "InscClaimAmtReimbursed",
    "DeductibleAmtPaid",
    "IPAnnualReimbursementAmt",
    "IPAnnualDeductibleAmt",
    "OPAnnualReimbursementAmt",
    "OPAnnualDeductibleAmt",
    "DurationofClaim",
    "NumberofDaysAdmitted"
]

for column in columns_to_transform:
    df_train1[f"PerBeneIDAvg_{column}"] = df_train1.groupby('BeneID')[column].transform('mean')
    df_test1[f"PerBeneIDAvg_{column}"] = df_test1.groupby('BeneID')[column].transform('mean')

    df_train1[f"PerAttendingPhysician Avg_{column}"] = df_train1.groupby('AttendingPhysician')[column].transform('mean')
    df_test1[f"PerAttendingPhysician Avg_{column}"] = df_test1.groupby('AttendingPhysician')[column].transform('mean')

df_train1.drop(columns=['ClmAdmitDiagnosisCode', 'Provider', 'State', 'Race', 'Gender', 'County', 'AdmissionDt', 'AttendingPhysician', 'OtherPhysician', 'OperatingPhysician',
                        'DischargeDt', 'ClaimID', 'ClaimEndDt', 'DiagnosisGroupCode', 'ClaimStartDt', 'BeneID', 'ClaimID'], axis=1, inplace=True)

df_test1.drop(columns=['ClmAdmitDiagnosisCode', 'State', 'Race', 'County', 'Gender', 'AdmissionDt', 'DiagnosisGroupCode', 'OperatingPhysician', 'DischargeDt', 'AttendingPhysician', 'OtherPhysician',
                       'ClaimID', 'ClaimEndDt', 'ClaimStartDt', 'ClaimID'], axis=1, inplace=True)

#fraud reported
px.histogram(df_train1, x="PotentialFraud",color='PotentialFraud',title="PotentialFraud", height=500, width=700)

px.histogram(df_train1, x="RenalDiseaseIndicator",color='PotentialFraud',title="RenalDiseaseIndicator", height=500, width=700)

px.histogram(df_train1, x="Admitted",color='PotentialFraud',title="Admitted", height=500, width=700)

px.histogram(df_train1, x="ClmProcedureCodeIndex",color='PotentialFraud',title="ChronicDiseaseIndex", height=500, width=700)

px.histogram(df_train1, x="ClmDiagnosisCodeIndex",color='PotentialFraud',title="ChronicDiseaseIndex", height=500, width=700)

df_train1['PotentialFraud'] = df_train1['PotentialFraud'].replace({'No':0, 'Yes': 1})

"""**Data Preprocessing**"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Split BEFORE feature engineering to prevent data leakage
df_train2 , df_val = train_test_split(df_train1, test_size=0.10, random_state=42, stratify=df_train1['PotentialFraud'])

# Now perform feature engineering on training data only
def create_aggregated_features(df):
    """Create aggregated features from training data to prevent data leakage"""
    # Per Provider averages
    provider_cols = ["InscClaimAmtReimbursed", "DeductibleAmtPaid", "IPAnnualReimbursementAmt",
                     "IPAnnualDeductibleAmt", "OPAnnualReimbursementAmt", "OPAnnualDeductibleAmt",
                     "Age", "NoOfMonths_PartACov", "NoOfMonths_PartBCov", "DurationofClaim", "NumberofDaysAdmitted"]

    for col in provider_cols:
        if 'Provider' in df.columns:
            df[f"PerProviderAvg_{col}"] = df.groupby('Provider')[col].transform('mean')

    # Per BeneID averages
    bene_cols = ["InscClaimAmtReimbursed", "DeductibleAmtPaid", "IPAnnualReimbursementAmt",
                 "IPAnnualDeductibleAmt", "OPAnnualReimbursementAmt", "OPAnnualDeductibleAmt",
                 "DurationofClaim", "NumberofDaysAdmitted"]

    for col in bene_cols:
        if 'BeneID' in df.columns:
            df[f"PerBeneIDAvg_{col}"] = df.groupby('BeneID')[col].transform('mean')

    # Per Attending Physician averages
    for col in bene_cols:
        if 'AttendingPhysician' in df.columns:
            df[f"PerAttendingPhysicianAvg_{col}"] = df.groupby('AttendingPhysician')[col].transform('mean')

    return df

# Apply feature engineering to training data
df_train2 = create_aggregated_features(df_train2)
df_val = create_aggregated_features(df_val)
df_test1 = create_aggregated_features(df_test1)

# Drop unnecessary columns
cols_to_drop = ['ClmAdmitDiagnosisCode', 'Provider', 'State', 'Race', 'Gender', 'County',
                'AdmissionDt', 'AttendingPhysician', 'OtherPhysician', 'OperatingPhysician',
                'DischargeDt', 'ClaimID', 'ClaimEndDt', 'DiagnosisGroupCode', 'ClaimStartDt', 'BeneID']

# Handle duplicate ClaimID in drop list
cols_to_drop = [col for col in cols_to_drop if col in df_train2.columns]

df_train2 = df_train2.drop(columns=cols_to_drop, axis=1)
df_val = df_val.drop(columns=cols_to_drop, axis=1)
df_test1 = df_test1.drop(columns=[col for col in cols_to_drop if col in df_test1.columns], axis=1)

y_train = df_train2.pop('PotentialFraud')
X_train = df_train2

y_val = df_val.pop('PotentialFraud')
X_val = df_val

X_test = df_test1

print("After fixing data leakage:")
print(f"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}, X_test shape: {X_test.shape}")

categorical_cols = [col for col in X_train.columns if col.startswith('ChronicCond_')]

encoder = OneHotEncoder(handle_unknown='ignore')
encoder.fit(X_train[categorical_cols])
encoded_data_train = encoder.transform(X_train[categorical_cols])
encoded_data_test = encoder.transform(X_test[categorical_cols])
encoded_data_val = encoder.transform(X_val[categorical_cols])

encoded_df_train = pd.DataFrame(encoded_data_train.toarray(), columns=encoder.get_feature_names_out())
encoded_df_test = pd.DataFrame(encoded_data_test.toarray(), columns=encoder.get_feature_names_out())
encoded_df_val = pd.DataFrame(encoded_data_val.toarray(), columns=encoder.get_feature_names_out())

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
X_val = X_val.reset_index(drop=True)

X_train = pd.concat([X_train.drop(categorical_cols, axis=1), encoded_df_train], axis=1)
X_test = pd.concat([X_test.drop(categorical_cols, axis=1), encoded_df_test], axis=1)
X_val = pd.concat([X_val.drop(categorical_cols, axis=1), encoded_df_val], axis=1)

# Apply SMOTE if available
if SMOTE_AVAILABLE:
    counter = Counter(y_train)
    print('Before SMOTE:', counter)
    smt = SMOTE(random_state=42)
    X_train, y_train = smt.fit_resample(X_train, y_train)
    counter = Counter(y_train)
    print('After SMOTE:', counter)
    
    # Remove any duplicates created by SMOTE
    train_df = pd.concat([X_train, pd.Series(y_train, name='target')], axis=1)
    train_df = train_df.drop_duplicates()
    y_train = train_df['target'].values
    X_train = train_df.drop('target', axis=1)
    print(f'After removing SMOTE duplicates: {X_train.shape[0]} samples')
else:
    print("SMOTE not available, proceeding without oversampling")

# Check for potential data leakage (e.g., identical rows in train and val)
duplicates_train_val = pd.concat([X_train, X_val]).duplicated().sum()
print(f"Number of duplicate rows between train and val: {duplicates_train_val}")

X_train.head()

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix

# Only import XGBoost if available
if XGB_AVAILABLE:
    import xgboost as xgb
if SHAP_AVAILABLE:
    import shap
if LIME_AVAILABLE:
    from lime.lime_tabular import LimeTabularExplainer

def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):
    """Evaluate model performance with multiple metrics"""
    model.fit(X_train, y_train)

    # Predictions
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)
    train_proba = model.predict_proba(X_train)[:, 1]
    val_proba = model.predict_proba(X_val)[:, 1]

    # Metrics
    try:
        train_auc = roc_auc_score(y_train, train_proba)
    except:
        train_auc = float('nan')  # Skip if too large
    
    metrics = {
        'Model': model_name,
        'Train Accuracy': accuracy_score(y_train, train_pred),
        'Val Accuracy': accuracy_score(y_val, val_pred),
        'Train Precision': precision_score(y_train, train_pred),
        'Val Precision': precision_score(y_val, val_pred),
        'Train Recall': recall_score(y_train, train_pred),
        'Val Recall': recall_score(y_val, val_pred),
        'Train F1': f1_score(y_train, train_pred),
        'Val F1': f1_score(y_val, val_pred),
        'Train AUC': train_auc,
        'Val AUC': roc_auc_score(y_val, val_proba)
    }

    return model, metrics

# Model comparison
models_to_compare = {
    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=15),
}

if XGB_AVAILABLE:
    models_to_compare['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')

models_to_compare['GradientBoosting'] = GradientBoostingClassifier(random_state=42)

print("=== Model Comparison ===")
model_results = []
trained_models = {}

for name, model in models_to_compare.items():
    try:
        trained_model, metrics = evaluate_model(model, X_train, X_val, y_train, y_val, name)
        trained_models[name] = trained_model
        model_results.append(metrics)
        print(f"  Train AUC = {metrics['Train AUC']:.4f}, Train F1 = {metrics['Train F1']:.4f}")
        print(f"  Val AUC = {metrics['Val AUC']:.4f}, Val F1 = {metrics['Val F1']:.4f}")
    except Exception as e:
        print(f"Error training {name}: {e}")

# Select best model (XGBoost if available, otherwise RandomForest)
if XGB_AVAILABLE and 'XGBoost' in trained_models:
    best_model_name = 'XGBoost'
elif 'RandomForest' in trained_models:
    best_model_name = 'RandomForest'
else:
    best_model_name = list(trained_models.keys())[0]  # Fallback to first available model

best_model = trained_models[best_model_name]
print(f"\nSelected best model: {best_model_name}")

print(f"\nSelected best model: {best_model_name}")

# Cross-validation evaluation (no hyperparameter tuning to save time)
cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='roc_auc')
print(f"Cross-validation AUC scores: {cv_scores}")
print(f"Mean CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Final evaluation
final_pred = best_model.predict(X_val)
final_proba = best_model.predict_proba(X_val)[:, 1]

print("\n=== Final Model Performance ===")
print(f"Accuracy: {accuracy_score(y_val, final_pred):.4f}")
print(f"Precision: {precision_score(y_val, final_pred):.4f}")
print(f"Recall: {recall_score(y_val, final_pred):.4f}")
print(f"F1-Score: {f1_score(y_val, final_pred):.4f}")
print(f"AUC: {roc_auc_score(y_val, final_proba):.4f}")

print("\nClassification Report:")
print(classification_report(y_val, final_pred))

import random
import joblib
import json

lime_explainer = LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Not Fraud', 'Fraud'],
    discretize_continuous=True
)

def make_sentence(feature, weight):
    templates_pos = [
        f"The feature '{feature}' strongly pushed this claim towards being fraudulent (weight {weight:.3f}).",
        f"A higher value of '{feature}' increased the fraud suspicion (impact {weight:.3f}).",
        f"Based on '{feature}', the model leaned more towards fraud (contribution {weight:.3f})."
    ]
    templates_neg = [
        f"The feature '{feature}' helped reduce fraud suspicion (weight {weight:.3f}).",
        f"A lower value of '{feature}' decreased the chance of fraud (impact {weight:.3f}).",
        f"Because of '{feature}', the claim looked less fraudulent (contribution {weight:.3f})."
    ]
    if weight > 0:
        return random.choice(templates_pos)
    else:
        return random.choice(templates_neg)

def explain_single_claim(data_row, top_n=3):
    fraud_proba = best_model.predict_proba(data_row.to_frame().T)[0,1]
    pred_class = best_model.predict(data_row.to_frame().T)[0]

    if pred_class == 0:
        return {"prediction": "Not Fraud", "fraud_probability": fraud_proba, "explanation": []}

    if LIME_AVAILABLE:
        exp = lime_explainer.explain_instance(
            data_row.values,
            best_model.predict_proba,
            num_features=top_n
        )
        explanation = [make_sentence(feat, weight) for feat, weight in exp.as_list()]
    else:
        # Fallback to SHAP if LIME not available
        explainer = shap.TreeExplainer(best_model)
        shap_values = explainer.shap_values(data_row.to_frame().T)
        # Simple explanation based on top features
        feature_importance = dict(zip(X_train.columns, abs(shap_values[1][0])))
        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:top_n]
        explanation = [f"Feature '{feat}' contributed significantly to the prediction" for feat, _ in top_features]

    return {"prediction": "Fraud", "fraud_probability": fraud_proba, "explanation": explanation}

def explain_batch_claims(dataframe, top_n=3):
    results = []
    for i in range(len(dataframe)):
        row = dataframe.iloc[i]
        results.append(explain_single_claim(row, top_n=top_n))
    return results

user_row = X_val.iloc[0].to_dict()
single_df = pd.DataFrame([user_row], columns=X_val.columns)

single_result = explain_single_claim(single_df.iloc[0])
print("\n=== User Single Claim ===")
print("Prediction:", single_result["prediction"])
print("Fraud Probability:", single_result["fraud_probability"])
for s in single_result["explanation"]:
    print("-", s)

# Example: Explain a batch of validation samples (first 5)
batch_df = X_val.head(5).copy()
batch_results = explain_batch_claims(batch_df)

print(f"\n=== Batch Claims Explanation (First 5 validation samples) ===")
for i, res in enumerate(batch_results):
    print(f"\nClaim {i}: {res['prediction']} (Prob: {res['fraud_probability']:.2f})")
    for s in res["explanation"]:
        print("-", s)

# Save model and artifacts
print("\n=== Saving Model and Artifacts ===")
joblib.dump(best_model, "fraud_model.pkl")

# Save LIME explainer if available
if LIME_AVAILABLE:
    try:
        lime_explainer = LimeTabularExplainer(
            X_train.values,
            feature_names=X_train.columns.tolist(),
            class_names=['Not Fraud', 'Fraud'],
            discretize_continuous=True
        )
        joblib.dump(lime_explainer, "lime_explainer.pkl")
        print("LIME explainer saved")
    except Exception as e:
        print(f"Failed to save LIME explainer: {e}")
        print("Skipping LIME explainer save")
else:
    print("LIME not available, skipping explainer save")

# Save feature columns
with open("columns.json", "w") as f:
    json.dump(list(X_train.columns), f)

# Save model metadata
model_metadata = {
    'model_type': best_model_name,
    'best_params': 'no hyperparameter tuning (default params)',
    'cv_auc': cv_scores.mean(),
    'val_auc': roc_auc_score(y_val, final_proba),
    'features': list(X_train.columns),
    'lime_available': LIME_AVAILABLE
}

with open("model_metadata.json", "w") as f:
    json.dump(model_metadata, f, indent=2)

print("Model and metadata saved for backend use.")
print(f"- fraud_model.pkl: Trained {best_model_name} model")
print(f"- columns.json: Feature column names")
print(f"- model_metadata.json: Model information and performance")

